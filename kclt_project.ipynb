{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a3becb3",
   "metadata": {},
   "source": [
    "# **Anomalous Flight Detection Using Open Source Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859d08f",
   "metadata": {},
   "source": [
    "## Extracting Files ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e801f415",
   "metadata": {},
   "source": [
    "Only run this block if you are loading the raw opensky data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c2f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tarfile\n",
    "import glob\n",
    "\n",
    "### ONLY RUN THIS BLOCK IF YOU NEED TO EXTRACT FILES AND MAKE THEM IN TO PARQUET\n",
    "\n",
    "# Config\n",
    "TAR_DIR = r\"your-directory-here\"   # folder with .tar files\n",
    "OUTPUT_DIR = r\"your-directory-here\"\n",
    "EXTRACT_DIR = r\"your-directory-here\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
    "\n",
    "# Charlotte bounding box\n",
    "lat_min, lat_max = 34.714, 35.714\n",
    "lon_min, lon_max = -81.443, -80.443\n",
    "\n",
    "# Columns to keep\n",
    "COLUMNS_TO_KEEP = ['icao24','callsign','time','lon','lat',\n",
    "                   'baroaltitude','onground','velocity','heading','vertrate']\n",
    "\n",
    "\n",
    "# Function to process one CSV file\n",
    "def process_csv(file_path):\n",
    "    try:\n",
    "        filtered_chunks = []\n",
    "\n",
    "        for chunk in pd.read_csv(file_path, usecols=COLUMNS_TO_KEEP, chunksize=500_000):\n",
    "            mask = (chunk['latitude'].between(lat_min, lat_max)) & (chunk['longitude'].between(lon_min, lon_max))\n",
    "            filtered_chunks.append(chunk[mask])\n",
    "\n",
    "        if filtered_chunks:\n",
    "            df_filtered = pd.concat(filtered_chunks)\n",
    "            base_name = os.path.basename(file_path).replace(\".csv\", \".parquet\")\n",
    "            output_file = os.path.join(OUTPUT_DIR, base_name)\n",
    "            df_filtered.to_parquet(output_file, engine='pyarrow', index=False)\n",
    "            print(f\"Saved {output_file} ({len(df_filtered)} flights)\")\n",
    "        else:\n",
    "            print(f\"No flights near Charlotte in {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed {file_path}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Find all tar files in the directory\n",
    "gz_files = glob.glob(os.path.join(EXTRACT_DIR, \"*.csv.tar\"))\n",
    "print(f\"Found {len(gz_files)} tar files.\")\n",
    "for file in gz_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file, compression=\"tar\")\n",
    "        parquet_path = file.replace(\".csv.tar\", \".parquet\")\n",
    "        df.to_parquet(parquet_path, index=False)\n",
    "        print(f\"Converted: {os.path.basename(file)} → {os.path.basename(parquet_path)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {file}: {e}\")\n",
    "\n",
    "print(\"All conversions completed.\")\n",
    "\n",
    "# Save on memory by only using the relevant Charlotte flights\n",
    "parquet_files = glob.glob(os.path.join(EXTRACT_DIR, \"*.parquet\"))\n",
    "filtered_chunks = []\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(file)\n",
    "    df_filtered = df[\n",
    "        (df[\"lat\"].between(lat_min, lat_max)) &\n",
    "        (df[\"lon\"].between(lon_min, lon_max))\n",
    "    ]\n",
    "    filtered_chunks.append(df_filtered)\n",
    "\n",
    "# Combine only filtered parts\n",
    "df_filtered_all = pd.concat(filtered_chunks, ignore_index=True)\n",
    "print(f\"Combined filtered DataFrame shape: {df_filtered_all.shape}\")\n",
    "df_filtered_all.to_csv(\"kclt_opensky_sept_2021_to_jun_2022.csv\")\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8405786",
   "metadata": {},
   "source": [
    "Here is the code I used to scrape the METAR database, I had to put which URL I wanted to scrape in here. They only allow up to a full month of codes to be pulled at once so I had to do multiple requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f16589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape historical METAR data to match with flights\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import sys\n",
    "\n",
    "URL = (\"\")\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                   \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                   \"Chrome/120.0.0.0 Safari/537.36\")\n",
    "}\n",
    "\n",
    "# Defining a function to grab the URL\n",
    "def fetch(url):\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=20)\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: network/request failed:\", e, file=sys.stderr)\n",
    "        return None\n",
    "    print(\"HTTP\", resp.status_code)\n",
    "    if resp.status_code != 200:\n",
    "        print(\"Response length:\", len(resp.text))\n",
    "        # print a bit of the returned page for debugging\n",
    "        print(\"Page snippet:\\n\", resp.text[:1000])\n",
    "        return None\n",
    "    return resp.text\n",
    "\n",
    "# Extract the METARS from the data\n",
    "def extract_metars_from_pre(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    # find <pre> blocks\n",
    "    pres = soup.find_all(\"pre\")\n",
    "    metars = []\n",
    "    for p in pres:\n",
    "        block = p.get_text(\"\\n\").strip()\n",
    "        for line in block.splitlines():\n",
    "            line = line.strip()\n",
    "            if line.upper().startswith((\"METAR \", \"SPECI \")):\n",
    "                metars.append(line)\n",
    "    return metars\n",
    "\n",
    "def extract_metars_by_regex(text):\n",
    "    # Capture any line that starts with METAR or SPECI\n",
    "    pattern = re.compile(r'(?m)^\\s*(?:METAR|SPECI)\\b.*$')\n",
    "    return [m.strip() for m in pattern.findall(text)]\n",
    "\n",
    "# Put everything together and call the main function\n",
    "def main():\n",
    "    html = fetch(URL)\n",
    "    if not html:\n",
    "        print(\"No HTML returned; aborting.\")\n",
    "        return\n",
    "\n",
    "    metars = extract_metars_from_pre(html)\n",
    "    if not metars:\n",
    "        # Search raw HTML with regex if it fails for whatever reason\n",
    "        metars = extract_metars_by_regex(html)\n",
    "\n",
    "    if not metars:\n",
    "        print(\"No METAR/SPECI lines found. Dumping a short snippet to help debug:\")\n",
    "        print(html[:4000])\n",
    "        return\n",
    "\n",
    "    # Print count and first 20 lines\n",
    "    print(f\"Found {len(metars)} METAR/SPECI lines. Showing up to first 20:\\n\")\n",
    "    for i, line in enumerate(metars[:20], start=1):\n",
    "        print(f\"{i:02d}: {line}\")\n",
    "\n",
    "    # Save all to a file for later processing\n",
    "    out_file = \"kclt_sep_06_2021.txt\"\n",
    "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(metars))\n",
    "    print(f\"\\nSaved {len(metars)} lines to {out_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34be2c2",
   "metadata": {},
   "source": [
    "In this section I converted all of the METAR codes to usable data and saved it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1587c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from metpy.io import parse_metar_to_dataframe\n",
    "\n",
    "# Folder path\n",
    "folder = Path(r\"C:\\Users\\Rob\\code_stuff\\interview_pp\\metar_data\")  # Windows raw string\n",
    "\n",
    "# Mapping from 3-letter month abbreviations to month numbers\n",
    "MONTH_MAP = {\n",
    "    \"jan\": 1, \"feb\": 2, \"mar\": 3, \"apr\": 4, \"may\": 5, \"jun\": 6,\n",
    "    \"jul\": 7, \"aug\": 8, \"sep\": 9, \"oct\": 10, \"nov\": 11, \"dec\": 12\n",
    "}\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "# Grab the files\n",
    "for path in folder.glob(\"*.txt\"):\n",
    "    fname = path.stem.lower() \n",
    "    \n",
    "    # Extract month abbreviation from filename\n",
    "    match = re.search(r'_(\\w{3})_\\d{1,2}_(\\d{4})$', fname)\n",
    "    if not match:\n",
    "        print(f\"Skipping {path.name} — name doesn't match pattern.\")\n",
    "        continue\n",
    "    \n",
    "    month_abbr, year_str = match.groups()\n",
    "    month = MONTH_MAP.get(month_abbr)\n",
    "    year = int(year_str)\n",
    "    \n",
    "    print(f\"Parsing {path.name} → month={month}, year={year}\")\n",
    "\n",
    "    # Read file text\n",
    "    text = path.read_text().strip()\n",
    "\n",
    "    # Split into individual lines\n",
    "    lines = [line.strip().rstrip('=') for line in text.splitlines() if line.strip()]\n",
    "    \n",
    "    for line in lines:\n",
    "        try:\n",
    "            df_block = parse_metar_to_dataframe(line, year=2022, month=month)\n",
    "            df_block[\"source_file\"] = path.name\n",
    "            all_dfs.append(df_block)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not parse line in {path.name}: {line} → {e}\")\n",
    "\n",
    "# Combine all DataFrames\n",
    "if all_dfs:\n",
    "    df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"\\n Parsed {len(df)} total METARs from {len(all_dfs)} lines.\")\n",
    "else:\n",
    "    df = pd.DataFrame()\n",
    "    print(\"No valid METAR files found.\")\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"metar_combined.csv\", index=False)\n",
    "print(\"Saved to metar_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0ae869",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451e409",
   "metadata": {},
   "source": [
    "## Flight data analysis and conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a957c1",
   "metadata": {},
   "source": [
    "Converting from metric units here since that's not the standard for aviation in America "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f07ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon\n",
    "import contextily as ctx\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Read in the data and wrangle it a little\n",
    "df = pd.read_csv(\"kclt_opensky_sept_2021_to_jun_2022.csv\")\n",
    "\n",
    "# Fill in some na values\n",
    "df[\"squawk\"] = df[\"squawk\"].fillna(\"Missing\")\n",
    "df[\"callsign\"] = df[\"callsign\"].fillna(\"Missing\")\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert to datetime for ease\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"time\"], unit = \"s\")\n",
    "df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "df[\"minute\"] = df[\"datetime\"].dt.minute\n",
    "df[\"day\"] = df[\"datetime\"].dt.day\n",
    "\n",
    "# Group flights by icao & time to find the time difference\n",
    "df.sort_values([\"icao24\", \"datetime\"])\n",
    "df[\"time_diff\"] = df.groupby(\"icao24\")[\"datetime\"].diff().dt.total_seconds().div(60)\n",
    "df[\"time_diff\"] = df[\"time_diff\"].fillna(0)\n",
    "\n",
    "# Convert all metrics to ft, kts\n",
    "meters_to_feet = 3.28084\n",
    "meters_per_s_to_knots = 1.94384\n",
    "\n",
    "df[\"baroaltitude\"] = df[\"baroaltitude\"] * meters_to_feet\n",
    "df[\"velocity\"] = df[\"velocity\"] * meters_per_s_to_knots\n",
    "df[\"vertrate\"] = df[\"vertrate\"] * meters_to_feet\n",
    "\n",
    "# Calculate the distance in feet from the airport\n",
    "ft_per_dg_lat = 364173.24\n",
    "airport_lat, airport_lon = 35.2140, -80.9431\n",
    "dlat = df['lat'] - airport_lat\n",
    "dlon = df['lon'] - airport_lon\n",
    "df['dist_from_KCLT_ft'] = np.sqrt(\n",
    "    (dlat * ft_per_dg_lat)**2 +\n",
    "    (dlon * ft_per_dg_lat * np.cos(np.radians(airport_lat)))**2\n",
    ")\n",
    "\n",
    "# Calculate the bearing to the airport\n",
    "lat1 = np.radians(airport_lat)\n",
    "lon1 = np.radians(airport_lon)\n",
    "lat2 = np.radians(df['lat'])\n",
    "lon2 = np.radians(df['lon'])\n",
    "\n",
    "dlon = lon2 - lon1\n",
    "\n",
    "x = np.sin(dlon) * np.cos(lat2)\n",
    "y = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(dlon)\n",
    "\n",
    "initial_bearing = np.degrees(np.arctan2(x, y))\n",
    "df['bearing_to_airport'] = (initial_bearing + 360) % 360\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcde93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f5ec8",
   "metadata": {},
   "source": [
    "Merging our METAR data in to the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb54c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dfs on datetime to get the weather data\n",
    "df_metar = pd.read_csv(\"metar_combined.csv\")\n",
    "df_metar[\"date_time\"] = pd.to_datetime(df_metar[\"date_time\"])\n",
    "df_metar = df_metar.sort_values(\"date_time\")\n",
    "df = df.sort_values(\"datetime\")\n",
    "df_merged = pd.merge_asof(\n",
    "    df,\n",
    "    df_metar,\n",
    "    left_on='datetime',\n",
    "    right_on='date_time',\n",
    "    direction='nearest',      # nearest METAR before or after\n",
    "    tolerance=pd.Timedelta('1h')  # max 1 hour difference\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf632cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the data a little more with the added METAR data\n",
    "\n",
    "cols_to_drop = [\n",
    " # Most of these had tons of NA vlaues\n",
    "        \"wind_gust\", \n",
    "        \"current_wx1\", \n",
    "        \"current_wx2\", \n",
    "        \"current_wx3\", \n",
    "        \"low_cloud_type\", \n",
    "        \"low_cloud_level\", \n",
    "        \"medium_cloud_type\", \n",
    "        \"medium_cloud_level\",\n",
    "        \"high_cloud_type\",\n",
    "        \"high_cloud_level\",\n",
    "        \"highest_cloud_type\",\n",
    "        \"highest_cloud_level\",\n",
    "        \"eastward_wind\",\n",
    "        \"northward_wind\",\n",
    "        \"source_file\",\n",
    "        \"Unnamed: 0\",\n",
    "        \"latitude\", # These are for METAR data\n",
    "        \"longitude\",\n",
    "]\n",
    "df_merged = df_merged.drop(columns=cols_to_drop)\n",
    "df_merged = df_merged.dropna()\n",
    "df_merged.info()\n",
    "df_merged.to_csv(\"kclt_opensky_and_metar_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e4967",
   "metadata": {},
   "source": [
    "## Creating Flight Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ecef5c",
   "metadata": {},
   "source": [
    "In order to visualize the airspace better I created a geopandas df of LineStrings grouped by time and icao24 with a threshold of 60 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443fa017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import contextily as ctx\n",
    "from shapely.geometry import LineString\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"kclt_opensky_and_metar_clean.csv\")\n",
    "# Convert to GeoDataFrame with line geometry per flight\n",
    "# Save the ids and time for identifying arrivals and departures to add back to the normal df\n",
    "flight_paths = []\n",
    "ids = []\n",
    "avg_vertrates = []\n",
    "altitudes = []\n",
    "start_time = []\n",
    "end_time = []\n",
    "\n",
    "# Define a time threshold of 1 hour\n",
    "time_threshold = 60\n",
    "\n",
    "for icao, icao_df in df.groupby(\"icao24\"):\n",
    "    icao_df = icao_df.sort_values(\"datetime\").reset_index(drop=True)\n",
    "    \n",
    "    # Identify where the time gap exceeds an hour\n",
    "    split_indices = [0]  # start of first segment\n",
    "    for i, td in enumerate(icao_df[\"time_diff\"]):\n",
    "        if td > time_threshold:\n",
    "            split_indices.append(i)\n",
    "    split_indices.append(len(icao_df))  # end of last segment\n",
    "\n",
    "    # Create LineStrings for each flight segment\n",
    "    for start, end in zip(split_indices[:-1], split_indices[1:]):\n",
    "        segment = icao_df.iloc[start:end]\n",
    "        if len(segment) > 1:  # need at least 2 points for a LineString\n",
    "            coords = list(zip(segment[\"lon\"], segment[\"lat\"], segment[\"baroaltitude\"]))\n",
    "            flight_paths.append(LineString(coords))\n",
    "            ids.append(icao)\n",
    "            avg_vertrates.append(segment.vertrate.mean())\n",
    "            altitudes.append(segment.baroaltitude.mean()) \n",
    "            start_time.append(segment.datetime.iloc[0])\n",
    "            end_time.append(segment.datetime.iloc[-1]) \n",
    "\n",
    "# Convert to gdf\n",
    "gdf = gpd.GeoDataFrame({\n",
    "    \"icao24\": ids,\n",
    "    \"avg_vertrate\": avg_vertrates,\n",
    "    \"avg_altitude\": altitudes,\n",
    "    \"start_time\" : start_time,\n",
    "    \"end_time\" : end_time\n",
    "    }, geometry=flight_paths, crs=\"EPSG:4326\")\n",
    "\n",
    "# Convert to Web Mercator for basemap\n",
    "gdf_web = gdf.to_crs(epsg=3857)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "gdf_web.plot(ax=ax, color=\"Cyan\", alpha=0.07, linewidth=0.06)\n",
    "\n",
    "# Add basemap\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.DarkMatterNoLabels)\n",
    "\n",
    "ax.set_axis_off()\n",
    "plt.savefig(\"flight_paths.png\", dpi=500, bbox_inches=\"tight\", transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bb3c96",
   "metadata": {},
   "source": [
    "## Attempt to define Departures, Arrivals andd Cruising flights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f5f252",
   "metadata": {},
   "source": [
    "Here I had to define a cutoff for what we considered cruising. Ideally this data would be provided or derived from arrival/destination information but that was not provided in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f79b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the start and end points\n",
    "def get_start_alt(geom):\n",
    "    return geom.coords[0][2] if len(geom.coords[0]) > 2 else None\n",
    "\n",
    "def get_end_alt(geom):\n",
    "    return geom.coords[-1][2] if len(geom.coords[-1]) > 2 else None\n",
    "\n",
    "gdf[\"alt_start\"] = gdf.geometry.apply(get_start_alt)\n",
    "gdf[\"alt_end\"] = gdf.geometry.apply(get_end_alt)\n",
    "gdf = gdf.dropna()\n",
    "\n",
    "\n",
    "# Determine arrivals and departures\n",
    "def classify_by_alt(row):\n",
    "    z_start = row[\"alt_start\"]\n",
    "    z_end = row[\"alt_end\"]\n",
    "    z_rate = row[\"avg_vertrate\"]\n",
    "    # Handle missing altitudes\n",
    "    if z_start is None or z_end is None:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    \n",
    "\n",
    "    # Cruising: stays above 15000 ft\n",
    "    if (z_start >= 15000) and (z_end >= 15000) or (abs(z_rate)<5):\n",
    "        return \"Cruising\"\n",
    "    # Arrival has a rapid descent\n",
    "    elif (z_rate <= -5):\n",
    "        return \"Arrival\"\n",
    "    # Departure has a rapid ascent\n",
    "    elif(z_rate >= 5):\n",
    "        return \"Departure\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "# Avg vertrate within 500ft/min\n",
    "gdf[\"flight_phase\"] = gdf.apply(classify_by_alt, axis=1)\n",
    "gdf.groupby(\"flight_phase\").size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65886146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "gdf.to_csv(\"flight_paths.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1333741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime and determine start times\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "gdf[\"start_time\"] = pd.to_datetime(gdf[\"start_time\"])\n",
    "gdf[\"end_time\"] = pd.to_datetime(gdf[\"end_time\"])\n",
    "\n",
    "# Build flights table\n",
    "flights = gdf[[\"icao24\", \"start_time\", \"end_time\", \"flight_phase\"]].copy()\n",
    "\n",
    "# Sort both by icao24 and time\n",
    "df = df.sort_values(\"datetime\").reset_index(drop=True)\n",
    "flights = flights.sort_values(\"start_time\").reset_index(drop=True)\n",
    "\n",
    "#Had to vectorize because this took forever\n",
    "merged = pd.merge_asof(\n",
    "    df,\n",
    "    flights,\n",
    "    by=\"icao24\",\n",
    "    left_on=\"datetime\",\n",
    "    right_on=\"start_time\",\n",
    "    direction=\"backward\",\n",
    "    allow_exact_matches=True\n",
    ")\n",
    "\n",
    "mask = merged[\"datetime\"] <= merged[\"end_time\"]\n",
    "merged.loc[~mask, \"flight_phase\"] = None  # points outside the segment get NaN\n",
    "\n",
    "\n",
    "# print(filtered)\n",
    "\n",
    "df = merged\n",
    "\n",
    "# Save new df\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48e84dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all of this for our later models\n",
    "cols_to_drop = [  \n",
    "        \"station_id\",\n",
    "        \"date_time\",\n",
    "        \"remarks\",\n",
    "        \"start_time\",\n",
    "        \"end_time\",\n",
    "        \"squawk\",\n",
    "        \"callsign\"\n",
    "]\n",
    "df_save = df.drop(columns=cols_to_drop)\n",
    "df_save = df_save.dropna()\n",
    "df_save.info()\n",
    "df_save.to_csv(\"model_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2d4c2d",
   "metadata": {},
   "source": [
    "I wanted to map by altitude here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16703ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins\n",
    "bin_size = 100\n",
    "gdf[\"altitude_bin\"] = (gdf[\"avg_altitude\"] // bin_size) * bin_size\n",
    "\n",
    "# Convert to Web Mercator for basemap\n",
    "gdf_web = gdf.to_crs(epsg=3857)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10), facecolor=\"black\")\n",
    "\n",
    "gdf_web.plot(\n",
    "    ax=ax,\n",
    "    column=\"altitude_bin\",\n",
    "    cmap=\"RdYlBu\",\n",
    "    linewidth=0.08,\n",
    "    alpha=0.08\n",
    ")\n",
    "\n",
    "# Add dark basemap\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.DarkMatterNoLabels)\n",
    "\n",
    "# Create colorbar with discrete ticks for each bin\n",
    "unique_bins = sorted(gdf_web[\"altitude_bin\"].unique())\n",
    "sm = plt.cm.ScalarMappable(\n",
    "    cmap=\"RdYlBu\", \n",
    "    norm=plt.Normalize(vmin=min(unique_bins), vmax=max(unique_bins))\n",
    ")\n",
    "sm._A = []\n",
    "tick_locs = [b for b in unique_bins if b % 4000 == 0]\n",
    "cbar = plt.colorbar(sm, ax=ax, ticks=tick_locs, shrink=.8)\n",
    "cbar.set_label(\"Altitude (ft)\", color=\"black\", fontsize=20)\n",
    "cbar.ax.yaxis.set_tick_params(color='black')\n",
    "cbar.ax.yaxis.set_tick_params(labelcolor='black')\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "# Configure plot\n",
    "ax.set_axis_off()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"flight_paths_by_alt.png\", dpi=500, bbox_inches=\"tight\", transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad4274f",
   "metadata": {},
   "source": [
    "Here we can change what we're plotting. Right now it's class B vs class A airspace but it can also produce the arrivals and departures graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed157d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "# Define colors\n",
    "phase_colors = {\n",
    "    \"Cruising\": \"#FF13F0\",       # unique color\n",
    "    \"Arrivals & Departures\": \"cyan\"  # combined color for arrivals/departures\n",
    "}\n",
    "\n",
    "# Filter data for plotting\n",
    "gdf_plotting = gdf.to_crs(epsg=3857)\n",
    "gdf_filtered = gdf_plotting[gdf_plotting[\"flight_phase\"].isin([\"Arrival\", \"Departure\", \"Cruising\"])]\n",
    "\n",
    "# Map color: arrivals/departures get same color, cruising stays separate\n",
    "def map_phase_color(phase):\n",
    "    if phase in [\"Arrival\", \"Departure\"]:\n",
    "        return phase_colors[\"Arrivals & Departures\"]\n",
    "    else:\n",
    "        return phase_colors[\"Cruising\"]\n",
    "\n",
    "colors = gdf_filtered[\"flight_phase\"].map(map_phase_color)\n",
    "\n",
    "# Plot flights\n",
    "ax = gdf_filtered.plot(\n",
    "    figsize=(12, 12),\n",
    "    linewidth=0.08,\n",
    "    alpha=0.08,\n",
    "    color=colors\n",
    ")\n",
    "\n",
    "# Add basemap\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.DarkMatterNoLabels)\n",
    "\n",
    "# Create legend with custom labels\n",
    "handles = [\n",
    "    Line2D([0], [0], color=phase_colors[\"Arrivals & Departures\"], lw=2, label=\"Class B Airspace\"),\n",
    "    Line2D([0], [0], color=phase_colors[\"Cruising\"], lw=2, label=\"Class A Airspace\")\n",
    "]\n",
    "\n",
    "ax.legend(handles=handles, loc='upper right', fontsize=20)\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.savefig(\"flight_paths_by_phase.png\", dpi=500, bbox_inches=\"tight\", transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8246e7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be4903d",
   "metadata": {},
   "source": [
    "## Analyzing the Data with Densities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75ebeaa",
   "metadata": {},
   "source": [
    "Getting an feel for the dataset with density graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ab7290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Define a function to easily plot a bunch at once for the PP\n",
    "def plot_dens(var, xlabel, xlimit):\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(\n",
    "        data=df,\n",
    "        x=var,      \n",
    "        fill=True,\n",
    "        color=\"blue\",\n",
    "        alpha=0.6,\n",
    "        linewidth=1.5\n",
    "    )\n",
    "\n",
    "    ticks = plt.yticks()[0]  # get default tick locations\n",
    "    plt.yticks(ticks, [f\"{t*100:.1f}%\" for t in ticks]) \n",
    "    plt.xlabel(xlabel, fontsize = 20)\n",
    "    plt.ylabel(\"Density\",fontsize = 20)\n",
    "    plt.xlim(xlimit[0], xlimit[1])\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plot_dens(\"baroaltitude\", \"Altitude (feet)\", (0,60000))\n",
    "plt.axvline(x=10000, color=\"red\", linestyle=\"--\", linewidth=2, label=\"x = 250\", alpha = .5)\n",
    "plt.savefig(\"altitude_density.png\", dpi=500, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c13e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dens(\"velocity\", \"Velocity (knots)\", (0,600))\n",
    "plt.axvline(x=250, color=\"red\", linestyle=\"--\", linewidth=2, label=\"x = 250\", alpha = .5)\n",
    "plt.savefig(\"velocity_density.png\", dpi=500, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43ec022",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dens(\"vertrate\", \"Vertical Climbing Rate (ft/s)\", (-80, 80))\n",
    "plt.savefig(\"vertrate_density.png\", dpi=500, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca5ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dens(\"bearing_to_airport\", \"Bearing (Degrees from Charlotte)\",(0,360))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a5d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"kclt_opensky_and_metar_clean.csv\")\n",
    "\n",
    "def plot_metar(data, xlabel):\n",
    "\n",
    "    plt.figure(figsize = (10,6))\n",
    "    plt.hist(data, bins=10, color=\"blue\", alpha=.5, edgecolor=\"black\")\n",
    "    plt.xlabel(xlabel, fontsize=24)\n",
    "    plt.xticks(fontsize = 16)\n",
    "    plt.yticks(fontsize = 16)\n",
    "    plt.ylabel('Count', fontsize =24)\n",
    "    plt.savefig(\"wind_dir.png\", dpi=500, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# plot_metar(df[\"air_temperature\"], \"Temperature (Degrees Celsius)\")\n",
    "# plot_metar(df[\"dew_point_temperature\"], \"Dew Point (Degrees Celsius)\")\n",
    "# plot_metar(df[\"wind_speed\"],  \"Wind Speed (Knots)\")\n",
    "plot_metar(df[\"wind_direction\"], \"Wind Direction (Degrees from true North)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7e8cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hexbin plot of velocity and altitude\n",
    "x = df['velocity']\n",
    "y = df['baroaltitude']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "hb = plt.hexbin(\n",
    "    x, y,\n",
    "    gridsize=500,       # number of hexagons along x-axis\n",
    "    cmap='plasma',\n",
    "    mincnt=1,\n",
    "    bins = \"log\"        \n",
    ")\n",
    "plt.colorbar(hb).set_label(\"Count (log scale)\", size = 14)\n",
    "plt.xlabel('Velocity (knots)', fontsize = 20)\n",
    "plt.ylabel('Barometric Altitude (ft)', fontsize = 20)\n",
    "plt.ylim(0,50000)\n",
    "plt.xlim(0,600)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.hlines(10000,0,600, linestyles = '--', colors = 'red', alpha = .8)\n",
    "\n",
    "plt.savefig(\"velocity_baro.png\", dpi=500, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f23f6d",
   "metadata": {},
   "source": [
    "## Bayesian Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64d8810",
   "metadata": {},
   "source": [
    "Start of the Bayesian network, I had to filter a ton of columns due to skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a2e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pgmpy.models import DiscreteBayesianNetwork\n",
    "from pgmpy.estimators import K2, HillClimbSearch, BayesianEstimator\n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Load data\n",
    "bn_data = pd.read_csv(\"model_data.csv\")\n",
    "flight_df = pd.read_csv(\"flight_paths.csv\")\n",
    "bn_df = bn_data.drop(columns=[\n",
    "                            # Remove unwanted columns\n",
    "                            'Unnamed: 0.1',\n",
    "                            'Unnamed: 0',\n",
    "                            'onground', \n",
    "                            'alert', \n",
    "                            'spi', \n",
    "                            'elevation', \n",
    "                            'visibility',  \n",
    "                            'current_wx2_symbol', \n",
    "                            'current_wx3_symbol',\n",
    "                            'cloud_coverage',\n",
    "                            'dist_from_KCLT_ft',\n",
    "                            \"icao24\",\n",
    "                            \"datetime\",\n",
    "                            \"lastcontact\",\n",
    "                            \"lastposupdate\",\n",
    "                            'day',\n",
    "                            'minute',\n",
    "                            'hour',\n",
    "                            'altimeter',\n",
    "                            'air_pressure_at_sea_level',\n",
    "                            'geoaltitude',\n",
    "                            'time',\n",
    "                            'air_temperature',\n",
    "                            'time_diff'                    \n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb0d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b961f6",
   "metadata": {},
   "source": [
    "Changing back to a discrete label for weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73b430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the weather symbols back to their discrete state\n",
    "bn_df[\"current_wx1_symbol\"].value_counts()\n",
    "wx_map = {\n",
    "    0: \"Clear\",\n",
    "    17: \"Fog\",\n",
    "    61: \"Rain\",\n",
    "    95: \"Thunderstorm\",\n",
    "    97: \"Thunderstorm\"\n",
    "}\n",
    "\n",
    "bn_df[\"wx_label\"] = bn_df[\"current_wx1_symbol\"].map(lambda x: wx_map.get(int(x % 100), \"Unknown\"))\n",
    "bn_df = bn_df.drop(columns=['current_wx1_symbol'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d927f7",
   "metadata": {},
   "source": [
    "Finding skewed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9feaba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "import pandas as pd\n",
    "\n",
    "# Compute skewness for all numeric columns\n",
    "skewness = bn_df.select_dtypes(include=['number']).apply(skew, nan_policy='omit')\n",
    "\n",
    "# Sort by absolute skewness\n",
    "skewness = skewness.sort_values(key=abs, ascending=False)\n",
    "\n",
    "skew_limit = 1.0  # can adjust for stricter filter\n",
    "skewed_cols = skewness[abs(skewness) > skew_limit].index.tolist()\n",
    "\n",
    "print(\"Highly skewed columns:\")\n",
    "print(skewed_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d21ed4",
   "metadata": {},
   "source": [
    "Discretizing our columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2030b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer, PowerTransformer\n",
    "from pgmpy.models import DiscreteBayesianNetwork\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "# Automatically select numeric columns except 'flight_phase'\n",
    "cols_to_discretize = bn_df.select_dtypes(include=['float64', 'int64']).columns.difference(['flight_phase','wx_label'])\n",
    "\n",
    "# Remove near-constant columns to avoid KBins warning\n",
    "skewed_cols = [col for col in skewed_cols if bn_df[col].nunique() > 1]\n",
    "\n",
    "# Initialize PowerTransformer\n",
    "pt = PowerTransformer(method='yeo-johnson', standardize=True)  # Yeo-Johnson works with negative values too\n",
    "\n",
    "# Fit and transform\n",
    "bn_df[skewed_cols] = pt.fit_transform(bn_df[skewed_cols])\n",
    "\n",
    "# Initialize KBinsDiscretizer\n",
    "kb = KBinsDiscretizer(n_bins=9, encode=\"ordinal\", strategy=\"quantile\")\n",
    "\n",
    "# Fit on the data\n",
    "kb.fit(bn_df[cols_to_discretize])\n",
    "\n",
    "# Save bin edges for mapping real values to bins later\n",
    "bin_edges = {col: kb.bin_edges_[i] for i, col in enumerate(cols_to_discretize)}\n",
    "\n",
    "# Transform the data\n",
    "bn_df[cols_to_discretize] = kb.transform(bn_df[cols_to_discretize])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3270537f",
   "metadata": {},
   "source": [
    "Making the Bayesian network. I had to define the initial structure. GES would sometimes ignore the required edges and forbidden edges but temporal fixed a lot of my issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb453131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "from pgmpy.estimators import BIC, GES, ExpertKnowledge\n",
    "\n",
    "expert_knowledge = ExpertKnowledge(\n",
    "    # Edges that must appear in the final DAG\n",
    "    required_edges=[('lat', 'bearing_to_airport'),\n",
    "                    ('lon', 'bearing_to_airport'),\n",
    "                    ('flight_phase', 'velocity'),\n",
    "                    ('baroaltitude', 'velocity'),\n",
    "                    ('vertrate','baroaltitude'),\n",
    "                    ('flight_phase', 'baroaltitude'),\n",
    "                    ('flight_phase', 'vertrate')\n",
    "                    \n",
    "    ],\n",
    "    \n",
    "    # Edges that must not appear\n",
    "    forbidden_edges=[\n",
    "\n",
    "    # Heading effects\n",
    "    ('heading', 'baroaltitude'),\n",
    "    ('heading', 'vertrate'),\n",
    "    ('bearing_to_airport','dew_point_temperature'), \n",
    "\n",
    "    # Baroaltitude effects\n",
    "    ('baroaltitude', 'flight_phase'),\n",
    "    ('baroaltitude', 'velocity'),\n",
    "\n",
    "    # Wind effects\n",
    "    ('wind_direction', 'lat'),\n",
    "    ('wind_direction', 'lon'),\n",
    "    ('wind_speed', 'lat'),\n",
    "    ('wind_speed', 'lon'),\n",
    "\n",
    "    # Dew point / weather effects\n",
    "    ('dew_point_temperature', 'lon'),\n",
    "    ('dew_point_temperature', 'lat')\n",
    "    ],\n",
    "    \n",
    "    # Order that edges must appear in\n",
    "    temporal_order=[\n",
    "    ('dew_point_temperature', 'wind_direction', 'wind_speed', 'wx_label'),  # weather first\n",
    "    ('lat', 'lon', 'bearing_to_airport'),  # position\n",
    "    ('flight_phase', 'baroaltitude', 'vertrate', 'velocity'),  # flight dynamics\n",
    "    ('heading',)  # heading last\n",
    "]\n",
    ")\n",
    "\n",
    "hc = GES(bn_df)\n",
    "dag = hc.estimate(scoring_method=BIC(bn_df), expert_knowledge=expert_knowledge)\n",
    "\n",
    "bn_model = DiscreteBayesianNetwork(dag.edges())\n",
    "bn_model.fit(bn_df, estimator=MaximumLikelihoodEstimator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c49c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the BN\n",
    "import pickle\n",
    "\n",
    "with open(\"bn_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(bn_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d93b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model.nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf42138",
   "metadata": {},
   "source": [
    "Testing a flight phase here for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8466ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that can map the real values of a flight to their corresponding bins\n",
    "import warnings\n",
    "infer = VariableElimination(bn_model)\n",
    "\n",
    "\n",
    "def map_to_bin(col, value):\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    if col in cols_to_discretize:\n",
    "        # Transform value only if column was included in PowerTransformer\n",
    "        if col in skewed_cols:\n",
    "            # Create placeholder for all skewed features (same length as fit)\n",
    "            tmp = np.zeros((1, len(skewed_cols)))\n",
    "            col_idx_skewed = list(skewed_cols).index(col)\n",
    "            tmp[0, col_idx_skewed] = value\n",
    "\n",
    "            # Transform and extract the corresponding transformed value\n",
    "            val_t = pt.transform(tmp)[0, col_idx_skewed]\n",
    "        else:\n",
    "            val_t = value  # leave untouched if not skewed\n",
    "        \n",
    "        # Use KBinsDiscretizer bin edges for this column\n",
    "        col_idx = list(cols_to_discretize).index(col)\n",
    "        edges = kb.bin_edges_[col_idx]\n",
    "        bin_idx = np.digitize(val_t, edges[1:-1])\n",
    "        return np.clip(bin_idx, 0, len(edges)-2)\n",
    "    else:\n",
    "        return value  # categorical or excluded variables\n",
    "\n",
    "\n",
    "# Define evidence using raw values\n",
    "evidence = {\n",
    "    'velocity': map_to_bin('velocity', 450),\n",
    "    'vertrate': map_to_bin('vertrate', 0),\n",
    "    'baroaltitude': map_to_bin('baroaltitude', 35000),\n",
    "    \n",
    "}\n",
    "\n",
    "query_result = infer.query(variables=[\"flight_phase\"], evidence=evidence)\n",
    "print(query_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df63aa18",
   "metadata": {},
   "source": [
    "Here I'm going to analyze what our Bayesian network finds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8723988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import warnings\n",
    "import pandas as pd\n",
    "bn_data = pd.read_csv(\"model_data.csv\")\n",
    "bn_data = bn_data.drop(columns=[\n",
    "                            # Remove unwanted columns\n",
    "                            'Unnamed: 0.1',\n",
    "                            'Unnamed: 0',\n",
    "                            'onground', \n",
    "                            'alert', \n",
    "                            'spi', \n",
    "                            'elevation', \n",
    "                            'visibility',  \n",
    "                            'current_wx2_symbol', \n",
    "                            'current_wx3_symbol',\n",
    "                            'cloud_coverage',\n",
    "                            'dist_from_KCLT_ft',\n",
    "                            \"icao24\",\n",
    "                            \"datetime\",\n",
    "                            \"lastcontact\",\n",
    "                            \"lastposupdate\",\n",
    "                            'day',\n",
    "                            'minute',\n",
    "                            'hour',\n",
    "                            'altimeter',\n",
    "                            'air_pressure_at_sea_level',\n",
    "                            'geoaltitude',\n",
    "                            'time',\n",
    "                            'air_temperature',\n",
    "                            'time_diff'  \n",
    "]\n",
    ")\n",
    "\n",
    "def map_to_bin(col, value):\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    if col in cols_to_discretize:\n",
    "        # Transform value only if column was included in PowerTransformer\n",
    "        if col in skewed_cols:\n",
    "            # Create placeholder for all skewed features (same length as fit)\n",
    "            tmp = np.zeros((1, len(skewed_cols)))\n",
    "            col_idx_skewed = list(skewed_cols).index(col)\n",
    "            tmp[0, col_idx_skewed] = value\n",
    "\n",
    "            # Transform and extract the corresponding transformed value\n",
    "            val_t = pt.transform(tmp)[0, col_idx_skewed]\n",
    "        else:\n",
    "            val_t = value  # leave untouched if not skewed\n",
    "        \n",
    "        # Use KBinsDiscretizer bin edges for this column\n",
    "        col_idx = list(cols_to_discretize).index(col)\n",
    "        edges = kb.bin_edges_[col_idx]\n",
    "        bin_idx = np.digitize(val_t, edges[1:-1])\n",
    "        return np.clip(bin_idx, 0, len(edges)-2)\n",
    "    else:\n",
    "        return value  # categorical or excluded variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f7eaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load your model\n",
    "with open(\"bn_model.pkl\", \"rb\") as f:\n",
    "    bn_model = pickle.load(f)\n",
    "\n",
    "# model.edges() gives parent → child relationships\n",
    "edges = list(bn_model.edges())\n",
    "print(\"Dependencies (Parent -> Child):\")\n",
    "for parent, child in edges:\n",
    "    print(f\"{parent} -> {child}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc905fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "bn_data[\"datetime\"] = pd.to_datetime(bn_data[\"datetime\"])\n",
    "\n",
    "\n",
    "\n",
    "# Filter flight\n",
    "icao24 = '06a052'\n",
    "start_time = \"2022-06-13 00:16:20\"\n",
    "end_time = \"2022-06-13 00:20:20\"\n",
    "\n",
    "flight_df = bn_data[\n",
    "    (bn_data['icao24'] == icao24) &\n",
    "    (bn_data[\"datetime\"] >= start_time) &\n",
    "    (bn_data[\"datetime\"] <= end_time)\n",
    "].sort_values(\"datetime\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Take last row, modify, and append\n",
    "last_row = flight_df.iloc[-1].copy()\n",
    "last_row[\"velocity\"] = last_row[\"velocity\"] - 500\n",
    "last_row[\"datetime\"] = last_row[\"datetime\"] + pd.Timedelta(seconds=10)\n",
    "\n",
    "flight_df = pd.concat([flight_df, last_row.to_frame().T], ignore_index=True)\n",
    "\n",
    "flight_df = flight_df.drop(columns=[\n",
    "                            # Remove unwanted columns\n",
    "                            'Unnamed: 0.1',\n",
    "                            'Unnamed: 0','onground', \n",
    "                            'alert', 'spi', 'elevation', \n",
    "                            'visibility', 'current_wx1_symbol', \n",
    "                            'current_wx2_symbol', \n",
    "                            'current_wx3_symbol',\n",
    "                            'cloud_coverage',\n",
    "                            'dist_from_KCLT_ft'\n",
    "                            \n",
    "])\n",
    "                        \n",
    "flight_df[\"datetime\"] = pd.to_datetime(flight_df[\"datetime\"])\n",
    "flight_df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e3285",
   "metadata": {},
   "source": [
    "I wanted to test the Bayesian net with a synthetic scenario to see how it picks up on anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236dfa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fake flight to demonstrate the rapid change in out anomaly score\n",
    "import pandas as pd\n",
    "\n",
    "n_maintain = 60      # steps maintaining altitude\n",
    "n_fall = 60           # steps in dive\n",
    "alt_start = 35000\n",
    "v_start = 500     # starting altitude in ft\n",
    "dv = 10\n",
    "dt = 1               # time step in seconds\n",
    "vertrate = -400\n",
    "\n",
    "\n",
    "\n",
    "# Velocity\n",
    "dummy_flight_1 = pd.DataFrame(columns = bn_df.columns)   \n",
    "dummy_flight_1[\"velocity\"] = [500] *(n_maintain + n_fall) \n",
    "\n",
    "# Vertical rate - flight just dive bombs\n",
    "dummy_flight_1[\"vertrate\"] = ([0] * n_maintain) + ([vertrate] * n_fall)\n",
    "\n",
    "\n",
    "# Altitude\n",
    "alt_end = []\n",
    "for i in range(n_fall):\n",
    "    alt_end.append(alt_start + (vertrate * i))\n",
    "dummy_flight_1[\"baroaltitude\"] = [alt_start] * (n_maintain) + alt_end\n",
    "dummy_flight_1['flight_phase'] = 'Cruising'\n",
    "dummy_flight_1['wx_label'] = 'Clear'\n",
    "dummy_flight_1['bearing_to_airport'] = 340\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba120960",
   "metadata": {},
   "source": [
    "Defining a compute anomaly score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5766ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "\n",
    "# Make a function that can compute the joint probability\n",
    "evidence_cols = ['baroaltitude', 'vertrate', 'velocity', 'flight_phase']\n",
    "\n",
    "epsilon = 1e-12  # to avoid log(0)\n",
    "\n",
    "# Initialize inference object\n",
    "infer = VariableElimination(bn_model)\n",
    "\n",
    "def compute_anomaly_score(row):\n",
    "    joint_prob = 1.0\n",
    "\n",
    "    for var in evidence_cols:\n",
    "        val = row[var]\n",
    "\n",
    "        # Map to bin\n",
    "        bin_value = map_to_bin(var, val)\n",
    "\n",
    "        # Build evidence excluding current variable\n",
    "        other_evidence = {v: map_to_bin(v, row[v]) for v in evidence_cols if v != var}\n",
    "\n",
    "        # Query probability of current variable given other evidence\n",
    "        res = infer.query(variables=[var], evidence=other_evidence)\n",
    "\n",
    "        # Find the index of the bin value in the CPD states\n",
    "        try:\n",
    "            state_idx = res.state_names[var].index(bin_value)\n",
    "        except ValueError:\n",
    "            # If somehow the value is outside known bins, assign a very small probability\n",
    "            joint_prob *= epsilon\n",
    "            continue\n",
    "\n",
    "        joint_prob *= res.values[state_idx]\n",
    "\n",
    "    return -np.log(joint_prob + epsilon)\n",
    "# Apply to each row\n",
    "dummy_flight_1['anomaly_score'] = dummy_flight_1.apply(compute_anomaly_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9d126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50k row sample for estimate on density\n",
    "bn_sample = bn_data.sample(50000)\n",
    "\n",
    "bn_sample['anomaly_score'] = bn_sample.apply(compute_anomaly_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "change = dummy_flight_1.iloc[60]['anomaly_score']\n",
    "orig = dummy_flight_1.iloc[0]['anomaly_score']\n",
    "final = dummy_flight_1.iloc[-1]['anomaly_score']\n",
    "\n",
    "first_jump = change - orig\n",
    "second_jump = final - orig\n",
    "std = second_jump/2.45\n",
    "\n",
    "print(first_jump)\n",
    "print(second_jump)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa664a8c",
   "metadata": {},
   "source": [
    "Flight path visualiztation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33179c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the flight path\n",
    "time  = range(120)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time, dummy_flight_1[\"baroaltitude\"], label = \"Flight Path\")\n",
    "plt.ylabel(\"Barometric Altitude\", fontsize = 20)\n",
    "plt.xlabel(\"Time in Seconds\", fontsize = 20)\n",
    "plt.tick_params(labelsize = 14)\n",
    "plt.legend()\n",
    "plt.savefig(\"dummy_path.png\", dpi=500)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836be792",
   "metadata": {},
   "source": [
    "Anomaly score visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e901e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the anomaly score\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dummy_flight_1['anomaly_score'])\n",
    "plt.ylabel('Anomaly Score', fontsize = 20)\n",
    "plt.xlabel('Time (seconds)', fontsize = 20)\n",
    "plt.grid(True, alpha = .3)\n",
    "plt.vlines(60, 0, 30, linestyles='--', colors = 'red', alpha = .3)\n",
    "plt.ylim(0,28)\n",
    "\n",
    "plt.savefig(\"anomaly.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46837dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bn_sample['anomaly_score'].quantile(.95))\n",
    "print(bn_sample['anomaly_score'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c9107d",
   "metadata": {},
   "source": [
    "Density of normal samples visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88dd7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the denisty of anomaly scores across normal conditions\n",
    "import seaborn as sns\n",
    "def plot_score(var, xlabel, xlimit):\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(\n",
    "        data=bn_sample,\n",
    "        x=var,      \n",
    "        fill=True,\n",
    "        color=\"blue\",\n",
    "        alpha=0.6,\n",
    "        linewidth=1.5\n",
    "    )\n",
    "    ticks = plt.yticks()[0]\n",
    "    plt.yticks(ticks, [f\"{t*100:.1f}%\" for t in ticks]) \n",
    "    plt.xlabel(xlabel, fontsize = 20)\n",
    "    plt.ylabel(\"Density\",fontsize = 20)\n",
    "    plt.xlim(xlimit[0], xlimit[1])\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plot_score(\"anomaly_score\", \"Anomaly Score\",(0,30))\n",
    "plt.savefig(\"anomaly_score_density.png\", dpi=500, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397c88b9",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5863b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"model_data.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f39ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanh is the way to go\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Get rid of unwanted columns\n",
    "cols_to_drop = [\n",
    "                'icao24',  \n",
    "                'alert', \n",
    "                'spi', \n",
    "                'datetime',\n",
    "                'Unnamed: 0',\n",
    "                'lastposupdate',\n",
    "                'lastcontact',\n",
    "                'hour',\n",
    "                'minute',\n",
    "                'day',\n",
    "                'current_wx1_symbol',\n",
    "                'current_wx2_symbol',\n",
    "                'current_wx3_symbol',\n",
    "                'geoaltitude',\n",
    "                'air_temperature',\n",
    "                'altimeter',\n",
    "                'elevation',\n",
    "                'visibility',\n",
    "                'cloud_coverage',\n",
    "                'onground',\n",
    "                'time_diff',\n",
    "                'dist_from_KCLT_ft',\n",
    "                'air_pressure_at_sea_level',\n",
    "                'time',\n",
    "                'flight_phase',\n",
    "                'Unnamed: 0.1'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "# Scale X\n",
    "X = df.drop(columns=cols_to_drop)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "features = X.columns\n",
    "\n",
    "X.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa55fe",
   "metadata": {},
   "source": [
    "Buildin a function to run multiple configs in succession and save their history, this was useful for running when I was away from my computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f2a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Build an autoencoder function to run multiple configs\n",
    "def build_autoencoder(input_dim, hidden_units1, hidden_units2, encoding_dim, l1=1e-4):\n",
    "    # Encoder\n",
    "    input_layer = keras.Input(shape=(input_dim,))\n",
    "    x = keras.layers.Dense(hidden_units1, activation='tanh')(input_layer)\n",
    "    x = keras.layers.Dense(hidden_units2, activation='tanh')(x)\n",
    "    encoded = keras.layers.Dense(encoding_dim, activation='tanh',\n",
    "                                 activity_regularizer=keras.regularizers.L1(l1))(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = keras.layers.Dense(hidden_units2, activation='tanh')(encoded)\n",
    "    x = keras.layers.Dense(hidden_units1, activation='tanh')(x)\n",
    "    decoded = keras.layers.Dense(input_dim, activation='linear')(x)\n",
    "\n",
    "    autoencoder = keras.Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder\n",
    "\n",
    "\n",
    "input_dim = X_scaled.shape[1]\n",
    "\n",
    "configs = [\n",
    "        (10, 8, 6, \"45\"),\n",
    "       \n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# save history for later\n",
    "for hidden_units1, hidden_units2, encoding_dim, name in configs:\n",
    "    autoencoder = build_autoencoder(input_dim, hidden_units1, hidden_units2, encoding_dim)\n",
    "    history = autoencoder.fit(\n",
    "        X_scaled, X_scaled,\n",
    "        epochs=10,\n",
    "        batch_size=64,\n",
    "        validation_split=0.3,\n",
    "        verbose=1\n",
    "    )\n",
    "    autoencoder.save(f\"autoencoder_{name}.keras\")\n",
    "    with open(f\"autoencoder_{name}_history.json\", \"w\") as f:\n",
    "        json.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c356d",
   "metadata": {},
   "source": [
    "Plotting the validation loss vs training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b12dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "hist = pd.read_json(\"autoencoder_45_history.json\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get loss values\n",
    "train_loss = hist[\"loss\"]\n",
    "val_loss = hist[\"val_loss\"]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss, label=\"Training Loss\", linewidth=2)\n",
    "plt.plot(val_loss, label=\"Validation Loss\", linewidth=2)\n",
    "\n",
    "plt.ylabel(\"Loss\", fontsize=20)\n",
    "plt.xlabel(\"Epochs\", fontsize=20)\n",
    "plt.legend(fontsize=14)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.grid(True, alpha = .3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800acfe0",
   "metadata": {},
   "source": [
    "I had to get the layer we were actually using here, since I ran different configs they all had different names. I passed a null vector through my autoencodedr to get the value. Latent space is the smallest layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad38e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model, Model\n",
    "import numpy as np\n",
    "\n",
    "# I didn't know how to save the latent layer name so this helped\n",
    "autoencoder = load_model(\"autoencoder_45.keras\")\n",
    "\n",
    "# Dummy input to get the layer name\n",
    "dummy_input = np.zeros((1, 10))\n",
    "\n",
    "# Run a forward pass to build layer shapes\n",
    "_ = autoencoder.predict(dummy_input)\n",
    "\n",
    "# Check shapes\n",
    "for layer in autoencoder.layers:\n",
    "    try:\n",
    "        intermediate_model = Model(inputs=autoencoder.input, outputs=layer.output)\n",
    "        output = intermediate_model.predict(dummy_input)\n",
    "        print(layer.name, output.shape)\n",
    "    except Exception as e:\n",
    "        print(layer.name, \"Error:\", e)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b570720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the encoder\n",
    "encoder = Model(\n",
    "    inputs=autoencoder.input,\n",
    "    outputs=autoencoder.get_layer(\"dense_8\").output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946b1ff",
   "metadata": {},
   "source": [
    "Making our mean vector and covariance matrix for Mahalanobis distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd399751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the Latent vector\n",
    "latent_X = encoder.predict(X_scaled)\n",
    "\n",
    "# Mean vector\n",
    "mean_vector = np.mean(latent_X, axis=0)\n",
    "\n",
    "# Covariance matrix\n",
    "cov_matrix = np.cov(latent_X, rowvar=False)\n",
    "\n",
    "# Inverse covariance matrix\n",
    "inv_cov_matrix = np.linalg.inv(cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c1f8b0",
   "metadata": {},
   "source": [
    "scipy has mahalanobis distance in it's package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fa891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "# Compute Mahalanobis Distances\n",
    "m_distances = np.array([\n",
    "    mahalanobis(row, mean_vector, inv_cov_matrix)\n",
    "    for row in latent_X\n",
    "])\n",
    "\n",
    "# Add distances to X_scaled DataFrame if you want\n",
    "X_df = pd.DataFrame(X_scaled, columns=X.columns)  # replace feature_names with your column names\n",
    "X_df['mahalanobis_distance'] = m_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ce8c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some columns to read easily\n",
    "cols_to_drop = [\n",
    "                 \n",
    "                'alert', \n",
    "                'spi', \n",
    "                'datetime',\n",
    "                'Unnamed: 0.1',\n",
    "                'Unnamed: 0',\n",
    "                'lastposupdate',\n",
    "                'lastcontact',\n",
    "                'hour',\n",
    "                'minute',\n",
    "                'day',\n",
    "                'current_wx1_symbol',\n",
    "                'current_wx2_symbol',\n",
    "                'current_wx3_symbol',\n",
    "                'geoaltitude',\n",
    "                'air_temperature',\n",
    "                'altimeter',\n",
    "                'elevation',\n",
    "                'visibility',\n",
    "                'cloud_coverage',\n",
    "                'onground',\n",
    "                'time_diff',\n",
    "                'dist_from_KCLT_ft',\n",
    "                'air_pressure_at_sea_level',\n",
    "                'flight_phase'\n",
    "]\n",
    "# df = df.drop(columns = cols_to_drop)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "df.nlargest(100,'mahalanobis_distance').loc[df['velocity'] > 400].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb4c791",
   "metadata": {},
   "source": [
    "Here I found some true anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c9473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by time again and take the values only an hour away\n",
    "df = df.sort_values([\"icao24\", \"time\"])\n",
    "df[\"time_diff\"] = df.groupby(\"icao24\")[\"time\"].diff().fillna(0)\n",
    "df[\"flight_id\"] = (df[\"time_diff\"] > 3600).cumsum() \n",
    "\n",
    "# Get the scores for flights in the 99th percentils\n",
    "flight_scores = (\n",
    "    df.groupby([\"icao24\", \"flight_id\"])[\"mahalanobis_distance\"]\n",
    "      .agg([\"mean\", \"max\", lambda x: np.percentile(x, 99)])\n",
    "      .rename(columns={\"<lambda_0>\": \"p99\"})\n",
    "      .reset_index()\n",
    ")\n",
    "flight_scores.rename(columns={\"<lambda_0>\": \"p99\"}, inplace=True)\n",
    "flight_scores = flight_scores.reset_index()\n",
    "flight_scores['icao24'].value_counts()\n",
    "cutoff_99 = np.percentile(df[\"mahalanobis_distance\"], 99)\n",
    "print(cutoff_99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793d9af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify some weird flights\n",
    "df['time'] = pd.to_datetime(df['time'], unit = 's')\n",
    "df.loc[(df['mahalanobis_distance'] >= 2) & (df['vertrate'] >= 15) & (df['icao24'] == 'a6f6dd')].tail(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57848543",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '2022-03-28 03:26:20'\n",
    "end = '2022-03-28 03:30:50'\n",
    "\n",
    "# Mean flight Mahalanobis Distance\n",
    "df_timeframe = df[(df['time'] >= start) & (df['time'] <= end) & (df['icao24'] == 'a6f6dd')]\n",
    "print(df_timeframe['mahalanobis_distance'].mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9756528",
   "metadata": {},
   "source": [
    "Plotting out abnormal flight path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b39639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "# Convert df_timeframe to GeoDataFrame\n",
    "flight_gdf = gpd.GeoDataFrame(\n",
    "    df_timeframe,\n",
    "    geometry=gpd.points_from_xy(df_timeframe[\"lon\"], df_timeframe[\"lat\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ").sort_values(\"time\")\n",
    "\n",
    "# Create a LineString for the flight\n",
    "flight_line = LineString(flight_gdf.geometry.tolist())\n",
    "flight_line_gdf = gpd.GeoDataFrame(\n",
    "    {\"icao24\": [df_timeframe[\"icao24\"].iloc[0]]},\n",
    "    geometry=[flight_line],\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Convert both gdf_plotting and flight_line_gdf to Web Mercator\n",
    "gdf_plotting = gdf.to_crs(epsg=3857)\n",
    "flight_line_gdf = flight_line_gdf.to_crs(epsg=3857)\n",
    "\n",
    "# Filter arrivals and departures\n",
    "mask = gdf_plotting[\"flight_phase\"].isin([\"Departure\"])\n",
    "gdf_filtered = gdf_plotting[mask]\n",
    "colors = gdf_filtered[\"flight_phase\"].map({\n",
    "    \n",
    "    \"Departure\": \"cyan\"\n",
    "})\n",
    "\n",
    "# Plot\n",
    "ax = gdf_filtered.plot(\n",
    "    figsize=(12, 12),\n",
    "    linewidth=0.08,\n",
    "    alpha=0.15,\n",
    "    color=colors\n",
    ")\n",
    "\n",
    "# Overlay your single flight path in bright red\n",
    "flight_line_gdf.plot(ax=ax, color=\"red\", linewidth=2, label=\"Selected Flight\")\n",
    "\n",
    "# Add basemap\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.DarkMatterNoLabels)\n",
    "\n",
    "# Add legend\n",
    "handles = [plt.Line2D([0], [0], color=color, lw=2, label=phase) \n",
    "           for phase, color in {\"Departure\": \"cyan\"}.items()]\n",
    "handles.append(plt.Line2D([0], [0], color=\"red\", lw=2, label=\"Selected Flight\"))\n",
    "ax.legend(handles=handles, loc='upper right', fontsize=20)\n",
    "\n",
    "ax.set_axis_off()\n",
    "plt.savefig(\"flight_paths_by_phase_with_selected.png\", dpi=500, bbox_inches=\"tight\", transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9085726",
   "metadata": {},
   "source": [
    "Extra analysis of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a14b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing aroundd with some plots to determine why a flight was anomalous\n",
    "def plot_flight(x,y):\n",
    "\n",
    "    plt.figure(figsize = (10,6))\n",
    "    plt.plot(x,y)\n",
    "    plt.xticks([])\n",
    "    plt.yticks()\n",
    "    plt.show()\n",
    "\n",
    "plot_flight(df_timeframe['time'],df_timeframe['vertrate'])\n",
    "plot_flight(df_timeframe['lat'],df_timeframe['lon'])\n",
    "plot_flight(df_timeframe['time'],df_timeframe['velocity'])\n",
    "plot_flight(df_timeframe['time'], df_timeframe['bearing_to_airport'])\n",
    "plot_flight(df_timeframe['time'], df_timeframe['baroaltitude'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf71f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plot of Mahalanobis Distance\n",
    "def plot_dens(var, xlabel, xlimit):\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(\n",
    "        data=X_df,\n",
    "        x=var,      \n",
    "        fill=True,\n",
    "        color=\"blue\",\n",
    "        alpha=0.6,\n",
    "        linewidth=1.5   \n",
    "    )\n",
    "    ticks = plt.yticks()[0]\n",
    "    plt.yticks(ticks, [f\"{t*100:.1f}%\" for t in ticks]) \n",
    "    plt.xlabel(xlabel, fontsize = 20)\n",
    "    plt.ylabel(\"Density\",fontsize = 20)\n",
    "    plt.xlim(xlimit[0], xlimit[1])\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.savefig(\"mahal_density.png\", dpi=500, bbox_inches=\"tight\")\n",
    "plot_dens(\"mahalanobis_distance\", \"Mahalanobis Distance\",(0,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd87e2b8",
   "metadata": {},
   "source": [
    "## Extra\n",
    "I was playing around with another fake flight, decided to go with a real one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8890d905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "n_maintain = 60      # steps maintaining altitude\n",
    "n_fall = 60          # steps in dive\n",
    "alt_start = 20000\n",
    "v_start = 500\n",
    "dv = 10\n",
    "dt = 1\n",
    "vertrate = -400\n",
    "\n",
    "icao_id = 'c06a7b'\n",
    "df_icao = df.loc[df['icao24'] == icao_id]\n",
    "df_icao = df_icao[:6]\n",
    "\n",
    "dummy_flight_2 = pd.DataFrame(columns=X.columns)\n",
    "\n",
    "# Only apply linspace to numeric columns\n",
    "for col in df_icao.columns:\n",
    "    if pd.api.types.is_numeric_dtype(df_icao[col]):\n",
    "        dummy_flight_2[col] = np.linspace(df_icao[col].min(), df_icao[col].max(), 120)\n",
    "    else:\n",
    "        # For non-numeric columns, just repeat the first value or fill with None\n",
    "        dummy_flight_2[col] = [df_icao[col].iloc[0]] * 120 if len(df_icao[col]) > 0 else [None] * 120\n",
    "\n",
    "# v_end = []\n",
    "# for i in range(n_fall):\n",
    "#     v_end.append(v_start + dv)    \n",
    "dummy_flight_2[\"velocity\"] = [500] *(n_maintain + n_fall) \n",
    "\n",
    "# Vertical rate\n",
    "dummy_flight_2[\"vertrate\"] = ([0] * n_maintain) + ([vertrate] * n_fall)\n",
    "\n",
    "\n",
    "# Altitude\n",
    "alt_end = []\n",
    "for i in range(n_fall):\n",
    "    alt_end.append(alt_start + (vertrate * i))\n",
    "dummy_flight_2[\"baroaltitude\"] = [alt_start] * (n_maintain) + alt_end\n",
    "dummy_flight_2['flight_phase_Arrival'] = False\n",
    "dummy_flight_2['flight_phase_Departure'] = False\n",
    "dummy_flight_2['flight_phase_Cruising'] = True\n",
    "dummy_flight_2 = dummy_flight_2.iloc[:, :10]\n",
    "dummy_flight_2.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd42f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "# Assume dummy_flight_2 is your new flight dataframe with same 10 features\n",
    "df_flight_scaled = scaler.transform(dummy_flight_2)  # same scaler used for X_scaled\n",
    "latent_flight = encoder.predict(df_flight_scaled)\n",
    "# Vectorized computation\n",
    "diff = latent_flight - mean_vector\n",
    "m_distances = np.array([\n",
    "    mahalanobis(x, mean_vector, inv_cov_matrix) \n",
    "    for x in latent_flight\n",
    "])\n",
    "\n",
    "# Add to dataframe\n",
    "dummy_flight_2['mahalanobis_distance'] = m_distances\n",
    "dummy_flight_2['mahalanobis_distance'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b594d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dummy_flight_2['mahalanobis_distance'])\n",
    "plt.ylabel(\"Mahalanobis Distance\", fontsize=20)\n",
    "plt.xlabel(\"Time (seconds)\", fontsize=20)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.grid(True, alpha = .3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4156ba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dummy_flight_2[X.columns].shape, X_scaled.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d56a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_scaled = scaler.transform(dummy_flight_2[X.columns])\n",
    "latent_dummy = encoder.predict(dummy_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8f802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get latent vectors for normal flights\n",
    "latent_normal = encoder.predict(X_scaled)  # all normal training samples\n",
    "\n",
    "# 2. Compute mean and covariance\n",
    "mu = np.mean(latent_normal, axis=0)\n",
    "cov = np.cov(latent_normal, rowvar=False) + np.eye(latent_normal.shape[1]) * 1e-6\n",
    "cov_inv = np.linalg.inv(cov)\n",
    "\n",
    "# 3. Compute latent for your test flight\n",
    "latent_test = encoder.predict(scaler.transform(dummy_flight_2[X.columns]))\n",
    "\n",
    "# 4. Mahalanobis distance per timestep\n",
    "diff = latent_test - mu\n",
    "mahal_distances = np.array([np.sqrt(np.dot(np.dot(d, cov_inv), d.T)) for d in diff])\n",
    "\n",
    "dummy_flight_2[\"mahalanobis\"] = mahal_distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cefe8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48e0689",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 6)\n",
    "icao_id = 'c06a7b'\n",
    "df_icao = df.loc[df['icao24'] == icao_id]\n",
    "df_icao = df_icao[:6]\n",
    "print(df_icao)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d123bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the encoder\n",
    "encoder = Model(\n",
    "    inputs=autoencoder.input,\n",
    "    outputs=autoencoder.get_layer(\"dense_57\").output\n",
    ")\n",
    "\n",
    "# Define a function to run a sample\n",
    "def run_sample(autoencoder, encoder, scaler, sample_df, features):\n",
    "    \n",
    "    # Scale the sample with the same function or it won't work\n",
    "    sample_scaled = scaler.transform(sample_df[features])\n",
    "    latent = encoder.predict(sample_scaled)\n",
    "    reconstructed = autoencoder.predict(sample_scaled)\n",
    "    error = np.mean(np.abs(sample_scaled - reconstructed), axis=1)\n",
    "    return latent, reconstructed, error\n",
    "\n",
    "sample_data = X.iloc[-1].to_frame().T\n",
    "features = list(X.columns)\n",
    "\n",
    "latent, reconstructed, error = run_sample(autoencoder, encoder, scaler, sample_data, features)\n",
    "reconstructed_original = scaler.inverse_transform(reconstructed)\n",
    "print(\"Latent:\", latent)\n",
    "print(\"Reconstructed:\", reconstructed_original)\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e60c12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "\n",
    "# Filter flight\n",
    "icao24 = 'fb1f38'\n",
    "start_time = \"2022-03-28 02:26:50\"\n",
    "end_time = \"2022-03-28 02:33:20\"\n",
    "\n",
    "flight_df = df[\n",
    "    (df['icao24'] == icao24) &\n",
    "    (df[\"datetime\"] >= start_time) &\n",
    "    (df[\"datetime\"] <= end_time)\n",
    "].sort_values(\"datetime\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "flight_df = pd.get_dummies(flight_df, columns=[\"flight_phase\"])\n",
    "\n",
    "flight_df[\"flight_phase_Arrival\"].head()\n",
    "\n",
    "new_cols = [\"flight_phase_Cruising\", \"flight_phase_Departure\"]\n",
    "flight_df[new_cols] = False\n",
    "\n",
    "# Take last row, modify, and append\n",
    "last_row = flight_df.iloc[-1].copy()\n",
    "last_row[\"velocity\"] = last_row[\"velocity\"] + 500\n",
    "last_row[\"datetime\"] = last_row[\"datetime\"] + pd.Timedelta(seconds=10)\n",
    "flight_df = pd.concat([flight_df, last_row.to_frame().T], ignore_index=True)\n",
    "\n",
    "# Scale the sample with the same function or it won't work\n",
    "sample_scaled = scaler.transform(flight_df[features])\n",
    "latent_vectors = encoder.predict(sample_scaled)\n",
    "\n",
    "mu = np.mean(latent_vectors, axis=0)          # mean vector\n",
    "cov = np.cov(latent_vectors, rowvar=False)   # covariance matrix\n",
    "cov += np.eye(cov.shape[0]) * 1e-6\n",
    "cov_inv = np.linalg.inv(cov)\n",
    "\n",
    "def mahalanobis_distance(x, mean, inv_cov):\n",
    "    diff = x - mean\n",
    "    return np.sqrt(np.dot(np.dot(diff, inv_cov), diff.T))\n",
    "\n",
    "mahal_distances = np.array([mahalanobis_distance(z, mu, cov_inv) for z in latent_vectors])\n",
    "\n",
    "flight_df[\"mahalanobis\"] = mahal_distances\n",
    "print(flight_df[[\"velocity\", \"datetime\", \"mahalanobis\"]].tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bbca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_df = pd.read_csv(\"flight_paths.csv\")\n",
    "paths_df[\"start_time\"] = pd.to_datetime(paths_df[\"start_time\"])\n",
    "paths_df[\"end_time\"] = pd.to_datetime(paths_df[\"end_time\"])\n",
    "paths_df[\"duration\"] = paths_df[\"end_time\"] - paths_df[\"start_time\"]\n",
    "\n",
    "one_hour = pd.Timedelta(hours=1)\n",
    "flights_around_one_hour = paths_df[\n",
    "    (paths_df[\"duration\"] >= one_hour) &\n",
    "    (paths_df[\"duration\"] <= one_hour + pd.Timedelta(minutes=30))  # optional ±5 min\n",
    "]\n",
    "\n",
    "# Check the result\n",
    "flights_around_one_hour[[\"icao24\", \"start_time\", \"end_time\", \"duration\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36459fed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
